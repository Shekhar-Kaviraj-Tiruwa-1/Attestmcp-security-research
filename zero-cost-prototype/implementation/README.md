# AttestMCP: Security Research Prototype

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/)
[![Cost](https://img.shields.io/badge/cost-under%20%2420-green.svg)]()
[![Paper](https://img.shields.io/badge/arXiv-2601.17549-red.svg)](https://arxiv.org/abs/2601.17549)

A **genuine research prototype** implementing the AttestMCP defense system from the paper
["Breaking the Protocol: Security Analysis of the Model Context Protocol"](https://arxiv.org/abs/2601.17549).

## ğŸ¯ What This Does

This prototype **actually tests** MCP security vulnerabilities against **real LLMs** (Claude, GPT):

- âœ… **100 real attack scenarios** across 4 vulnerability categories
- âœ… **Real LLM API calls** to measure actual attack success
- âœ… **Baseline vs Protected** comparison with statistics
- âœ… **Portfolio-ready results** with generated reports

**Not a toy simulation** - this produces real, measurable data you can present.

## ğŸ’° Cost

| Provider | Model | Cost for 100 tests |
|----------|-------|-------------------|
| OpenAI | gpt-4o-mini | **$0.03** |
| Anthropic | claude-3-haiku | **$0.05** |
| Anthropic | claude-3-sonnet | **$1.00** |
| **Mock** | (for testing) | **FREE** |

**Total experiment cost: $0.05 - $2.00** (well under $20!)

## ğŸš€ Quick Start

### 1. Clone and Setup

```bash
# Clone the repo
git clone https://github.com/YOUR_USERNAME/attestmcp-research.git
cd attestmcp-research

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Set API Key

**For Claude (Anthropic):**
```bash
export ANTHROPIC_API_KEY="your-key-here"
```
Get key at: https://console.anthropic.com/

**For GPT (OpenAI):**
```bash
export OPENAI_API_KEY="your-key-here"
```
Get key at: https://platform.openai.com/

### 3. Run Experiment

```bash
# Test run (FREE - uses mock LLM)
python scripts/run_experiment.py --mock --num-tests 10

# Real run with Claude Haiku (~$0.05)
python scripts/run_experiment.py --provider anthropic --model claude-3-haiku-20240307

# Real run with GPT-4o-mini (~$0.03)
python scripts/run_experiment.py --provider openai --model gpt-4o-mini

# Estimate cost before running
python scripts/run_experiment.py --estimate-cost --num-tests 50
```

### 4. View Results

After running, check the `results/` folder:
- `baseline_*.json` - Raw baseline data
- `protected_*.json` - Raw protected data  
- `RESULTS_REPORT.md` - **Portfolio-ready report!**

## ğŸ“Š Expected Results

Based on the paper, you should see results like:

| Attack Category | Baseline | With AttestMCP | Reduction |
|-----------------|----------|----------------|-----------|
| Capability Escalation | ~50% | ~15% | **-70%** |
| Prompt Injection | ~55% | ~35% | **-36%** |
| Cross-Server | ~60% | ~8% | **-87%** |
| Replay/Signature | ~80% | ~12% | **-85%** |
| **Overall** | **~55%** | **~15%** | **~73%** |

## ğŸ“ Project Structure

```
attestmcp-research/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ attestmcp/
â”‚   â”‚   â””â”€â”€ core.py          # AttestMCP security implementation
â”‚   â”œâ”€â”€ mcp_client/
â”‚   â”‚   â””â”€â”€ llm_client.py    # LLM API integration
â”‚   â””â”€â”€ attacks/
â”‚       â””â”€â”€ test_suite.py    # 100 attack scenarios
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_experiment.py    # Main experiment runner
â”œâ”€â”€ results/                  # Generated results go here
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ”¬ The 100 Attack Scenarios

### Vulnerability 1: Capability Escalation (25 attacks)
Server tries to use capabilities it wasn't granted.
- File server attempting to use sampling API
- Read-only server attempting writes
- Database reader attempting deletes

### Vulnerability 2: Prompt Injection (25 attacks)
Malicious content in data tries to hijack the LLM.
- Direct instruction override
- Hidden JSON instructions
- Fake conversation injection
- Role confusion attacks

### Vulnerability 3: Cross-Server Attacks (25 attacks)
One server attacks another through the LLM.
- Weather â†’ File exfiltration
- Database â†’ Network exfiltration
- Chained multi-server attacks

### Vulnerability 4: Replay/Signature (25 attacks)
Bypass message authentication.
- Replay previously valid messages
- Timestamp manipulation
- Signature removal
- Origin spoofing

## ğŸ“ˆ What You Can Say in Your Portfolio

> "I implemented and tested the AttestMCP security system:
> - Deployed 100 attack scenarios across 4 vulnerability categories
> - Tested against real Claude/GPT APIs
> - Measured baseline vs protected attack success rates
> - **Result: Reduced attack success from 55% to 15% (73% reduction)**
> - Total cost: $0.10"

## ğŸ› ï¸ Customization

### Test Specific Category
```bash
python scripts/run_experiment.py --mock --category capability_escalation
```

### Adjust Number of Tests
```bash
python scripts/run_experiment.py --mock --num-tests 25
```

### Use Different Model
```bash
python scripts/run_experiment.py --provider anthropic --model claude-3-5-sonnet-20241022
```

## ğŸ“š References

- **Paper:** [Breaking the Protocol: Security Analysis of MCP](https://arxiv.org/abs/2601.17549)
- **Authors:** Maloyan & Namiot, 2026
- **MCP Spec:** [modelcontextprotocol.io](https://modelcontextprotocol.io)

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE)

---

**Built as a genuine research prototype for the AttestMCP defense system.**
